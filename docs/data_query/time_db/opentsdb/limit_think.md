# OpenTSDB 生产应用与思考

OpenTSDB 在数据量小时是可用的，在千万级、亿级中提取几万条数据，比如某个指标半年内的5分钟级别的数据，还是很快响应的。但如果再提取多点数据，几十万，百万这样的量级，又或者提取后再做个聚合运算，OpenTSDB 就勉为其难：

- OpenTSDB 目前还是**单点做聚合运算**
- 这样的量级数据从HBase 中提取到单节点内存中进行聚合运算，在资源消耗方面不可忽视
- 一个查询一旦提取的量级大，OpenTSDB 向HBase 发起RPC 请求(OpenTSDB 一次请求默认128行)次数也必然增加，十几秒、几十秒的响应时间，这就限制了它的应用场景

## 架构中应用OpenTSDB 

需要事先考虑

- OpenTSDB 只有4 张HBase 表，其中一张是存放数据。**所有的数据都存放在一张表**，这就意味应用在OpenTSDB 这个层级上是无法更小的粒度来区别对待不同业务。

- OpenTSDB 不支持二级索引，只有一个基于HBase的RowKey。结合业务场景的查询维度，设计好RowKey 是应用好OpenTSDB 的关键！提前评估好metric + tag 背后扫描的数据量。**比如将high-cardinality tag 调整到metric 中，减少扫描的数据量**。

- OpenTSDB 能实时聚合计算功能，但基于单点运算能力有限，建议这种**聚合在入库阶段完成的**。比如将1 分钟粒度聚合、5分钟粒度聚合提前通过KafkaStream，Spark 等运算，将聚会结果存入OpenTSDB 供查询。

- Tcollector采集数据上报给OpenTSDB，建议在中间**加一层Kafka**。一来解偶两者之间的强依赖性，同时保存一段时间采集数据；二来可以对采集的原始数据进行二次加工再入OpenTSDB，如粒度聚合运算。

- 设计时需要结合查询场景，在查询性能和后期热点运维方面，做一个权衡考虑。如果**写入吞吐量大的话，建议开启salt**，但桶的个数不易太多，2-4个即可，默认是20 个桶。



### 部署运维

- 生产部署时，往往是多台机器部署，每台机器部署多个实例，读写分离。
- 设置tsd.storage.hbase.prefetch_meta = true(默认是false)，否则极端情况下会打爆hbase:meta 表请求。
- 2.2 版本开始，tsd 写数据到HBase有两种方式，一种是每来一条数据append 到hbase。另一种是**先缓存大量数据到TSD 的内存里，然后进行compaction？？？**，一次性写入。
- TSDB 在**每小时整点的时候将上个小时的数据读出来**(get)，然后compact 成一个row，写入(put)到HBase，然后删除(delete)原始数据，目的是减少存储消耗，增大了HBase 压力，性能平稳上出现一定的波动。